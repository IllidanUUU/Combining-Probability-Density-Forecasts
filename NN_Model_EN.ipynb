{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is used to train NN models and write result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 6)\n",
    "import tensorflow as tf\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Dense, Input, BatchNormalization\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "from scipy.stats import linregress\n",
    "probit = np.array([-1.28155, -0.84162, -0.52440, -0.25335])\n",
    "xx = np.concatenate((probit, np.array([0]), -np.flip(probit, axis = -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(input_shape):\n",
    "    X_input = Input(shape = input_shape)\n",
    "    X = Dense(32, activation='relu')(X_input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dense(32, activation='relu')(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    X = Dense(9)(X)\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pinball(y_true, y_pred):\n",
    "    tao = K.cast(np.reshape((np.array(range(9)) + 1)/10, (-1,1)), 'float32')\n",
    "    pin = K.dot(K.maximum(y_true - y_pred, 0), tao) + K.dot(K.maximum(y_pred - y_true, 0), 1. - tao)\n",
    "    return K.mean(pin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### region = region name, j  = index of the current model (j = 1, 2, ..., 5 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ME NH VT RI SEMASS WCMASS NEMASSBOST\n",
    "region = 'NEMASSBOST'\n",
    "j=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and write result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Tianyi/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8731 samples, validate on 840 samples\n",
      "Epoch 1/220\n",
      "8731/8731 [==============================] - 1s 154us/step - loss: 5.2490 - val_loss: 5.0405\n",
      "Epoch 2/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 4.8041 - val_loss: 4.7338\n",
      "Epoch 3/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 4.4162 - val_loss: 4.6113\n",
      "Epoch 4/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 4.0692 - val_loss: 4.5843\n",
      "Epoch 5/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 3.7444 - val_loss: 4.6212\n",
      "Epoch 6/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 3.4469 - val_loss: 4.8164\n",
      "Epoch 7/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 3.1708 - val_loss: 5.0376\n",
      "Epoch 8/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 2.9396 - val_loss: 5.1490\n",
      "Epoch 9/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 2.7423 - val_loss: 5.2284\n",
      "Epoch 10/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 2.5691 - val_loss: 5.3242\n",
      "Epoch 11/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 2.4235 - val_loss: 5.3575\n",
      "Epoch 12/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 2.2924 - val_loss: 5.3912\n",
      "Epoch 13/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 2.1766 - val_loss: 5.3108\n",
      "Epoch 14/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 2.0757 - val_loss: 5.2452\n",
      "Epoch 15/220\n",
      "8731/8731 [==============================] - 0s 4us/step - loss: 1.9897 - val_loss: 5.2144\n",
      "Epoch 16/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.9151 - val_loss: 5.1551\n",
      "Epoch 17/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.8459 - val_loss: 5.0288\n",
      "Epoch 18/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.7894 - val_loss: 4.8923\n",
      "Epoch 19/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.7365 - val_loss: 4.6901\n",
      "Epoch 20/220\n",
      "8731/8731 [==============================] - 0s 6us/step - loss: 1.6894 - val_loss: 4.5006\n",
      "Epoch 21/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.6501 - val_loss: 4.3477\n",
      "Epoch 22/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.6126 - val_loss: 4.1814\n",
      "Epoch 23/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.5763 - val_loss: 3.9984\n",
      "Epoch 24/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.5502 - val_loss: 3.8077\n",
      "Epoch 25/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.5243 - val_loss: 3.6849\n",
      "Epoch 26/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.4953 - val_loss: 3.5559\n",
      "Epoch 27/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.4747 - val_loss: 3.4170\n",
      "Epoch 28/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.4518 - val_loss: 3.1877\n",
      "Epoch 29/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.4311 - val_loss: 3.1149\n",
      "Epoch 30/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.4071 - val_loss: 2.9461\n",
      "Epoch 31/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.3869 - val_loss: 2.8884\n",
      "Epoch 32/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.3700 - val_loss: 2.6879\n",
      "Epoch 33/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.3553 - val_loss: 2.6011\n",
      "Epoch 34/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.3432 - val_loss: 2.5198\n",
      "Epoch 35/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.3316 - val_loss: 2.4286\n",
      "Epoch 36/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.3146 - val_loss: 2.3703\n",
      "Epoch 37/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.3055 - val_loss: 2.2389\n",
      "Epoch 38/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2961 - val_loss: 2.1942\n",
      "Epoch 39/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2823 - val_loss: 2.1046\n",
      "Epoch 40/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2719 - val_loss: 2.0424\n",
      "Epoch 41/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2643 - val_loss: 2.0214\n",
      "Epoch 42/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2556 - val_loss: 1.9891\n",
      "Epoch 43/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2454 - val_loss: 1.9146\n",
      "Epoch 44/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2310 - val_loss: 1.8921\n",
      "Epoch 45/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2273 - val_loss: 1.8378\n",
      "Epoch 46/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2225 - val_loss: 1.8015\n",
      "Epoch 47/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2117 - val_loss: 1.8031\n",
      "Epoch 48/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2066 - val_loss: 1.7587\n",
      "Epoch 49/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.2012 - val_loss: 1.7057\n",
      "Epoch 50/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1954 - val_loss: 1.6661\n",
      "Epoch 51/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1883 - val_loss: 1.6513\n",
      "Epoch 52/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1842 - val_loss: 1.6336\n",
      "Epoch 53/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1770 - val_loss: 1.6210\n",
      "Epoch 54/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1771 - val_loss: 1.5903\n",
      "Epoch 55/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1673 - val_loss: 1.5823\n",
      "Epoch 56/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1621 - val_loss: 1.5743\n",
      "Epoch 57/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1587 - val_loss: 1.5313\n",
      "Epoch 58/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1534 - val_loss: 1.5324\n",
      "Epoch 59/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1491 - val_loss: 1.5089\n",
      "Epoch 60/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1449 - val_loss: 1.4948\n",
      "Epoch 61/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1429 - val_loss: 1.4644\n",
      "Epoch 62/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1410 - val_loss: 1.4778\n",
      "Epoch 63/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1349 - val_loss: 1.4208\n",
      "Epoch 64/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1367 - val_loss: 1.3972\n",
      "Epoch 65/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1289 - val_loss: 1.3717\n",
      "Epoch 66/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1251 - val_loss: 1.3751\n",
      "Epoch 67/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1274 - val_loss: 1.3490\n",
      "Epoch 68/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1213 - val_loss: 1.3429\n",
      "Epoch 69/220\n",
      "8731/8731 [==============================] - 0s 4us/step - loss: 1.1178 - val_loss: 1.3281\n",
      "Epoch 70/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1139 - val_loss: 1.3331\n",
      "Epoch 71/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1147 - val_loss: 1.3068\n",
      "Epoch 72/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1145 - val_loss: 1.3196\n",
      "Epoch 73/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1141 - val_loss: 1.2924\n",
      "Epoch 74/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1088 - val_loss: 1.3063\n",
      "Epoch 75/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1031 - val_loss: 1.2720\n",
      "Epoch 76/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1029 - val_loss: 1.2895\n",
      "Epoch 77/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1000 - val_loss: 1.2662\n",
      "Epoch 78/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0949 - val_loss: 1.2520\n",
      "Epoch 79/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.1018 - val_loss: 1.2503\n",
      "Epoch 80/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0908 - val_loss: 1.2511\n",
      "Epoch 81/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0921 - val_loss: 1.2391\n",
      "Epoch 82/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0869 - val_loss: 1.2664\n",
      "Epoch 83/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0878 - val_loss: 1.2399\n",
      "Epoch 84/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0842 - val_loss: 1.2465\n",
      "Epoch 85/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0835 - val_loss: 1.2415\n",
      "Epoch 86/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0809 - val_loss: 1.2260\n",
      "Epoch 87/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0800 - val_loss: 1.2288\n",
      "Epoch 88/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0798 - val_loss: 1.2284\n",
      "Epoch 89/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0793 - val_loss: 1.2225\n",
      "Epoch 90/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0740 - val_loss: 1.2173\n",
      "Epoch 91/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0720 - val_loss: 1.2364\n",
      "Epoch 92/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0708 - val_loss: 1.2211\n",
      "Epoch 93/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0743 - val_loss: 1.2131\n",
      "Epoch 94/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0709 - val_loss: 1.2005\n",
      "Epoch 95/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0711 - val_loss: 1.2275\n",
      "Epoch 96/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0669 - val_loss: 1.2204\n",
      "Epoch 97/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0640 - val_loss: 1.2426\n",
      "Epoch 98/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0658 - val_loss: 1.2348\n",
      "Epoch 99/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0579 - val_loss: 1.2367\n",
      "Epoch 100/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0605 - val_loss: 1.2162\n",
      "Epoch 101/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0614 - val_loss: 1.2291\n",
      "Epoch 102/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0573 - val_loss: 1.2605\n",
      "Epoch 103/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0547 - val_loss: 1.2333\n",
      "Epoch 104/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0527 - val_loss: 1.2308\n",
      "Epoch 105/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0498 - val_loss: 1.2217\n",
      "Epoch 106/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0513 - val_loss: 1.2096\n",
      "Epoch 107/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0497 - val_loss: 1.2531\n",
      "Epoch 108/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0459 - val_loss: 1.2561\n",
      "Epoch 109/220\n",
      "8731/8731 [==============================] - 0s 6us/step - loss: 1.0473 - val_loss: 1.2226\n",
      "Epoch 110/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0442 - val_loss: 1.2176\n",
      "Epoch 111/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0423 - val_loss: 1.2204\n",
      "Epoch 112/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0426 - val_loss: 1.2328\n",
      "Epoch 113/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0399 - val_loss: 1.2245\n",
      "Epoch 114/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0304 - val_loss: 1.2220\n",
      "Epoch 115/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0391 - val_loss: 1.2173\n",
      "Epoch 116/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0370 - val_loss: 1.2036\n",
      "Epoch 117/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0356 - val_loss: 1.2158\n",
      "Epoch 118/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0349 - val_loss: 1.2168\n",
      "Epoch 119/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0294 - val_loss: 1.2382\n",
      "Epoch 120/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0280 - val_loss: 1.2154\n",
      "Epoch 121/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0300 - val_loss: 1.2051\n",
      "Epoch 122/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0212 - val_loss: 1.2162\n",
      "Epoch 123/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0210 - val_loss: 1.2198\n",
      "Epoch 124/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0193 - val_loss: 1.2066\n",
      "Epoch 125/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0144 - val_loss: 1.2109\n",
      "Epoch 126/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0200 - val_loss: 1.2008\n",
      "Epoch 127/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0146 - val_loss: 1.2178\n",
      "Epoch 128/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0169 - val_loss: 1.2233\n",
      "Epoch 129/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0127 - val_loss: 1.2334\n",
      "Epoch 130/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0122 - val_loss: 1.2246\n",
      "Epoch 131/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0123 - val_loss: 1.2463\n",
      "Epoch 132/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0162 - val_loss: 1.2200\n",
      "Epoch 133/220\n",
      "8731/8731 [==============================] - 0s 4us/step - loss: 1.0071 - val_loss: 1.2335\n",
      "Epoch 134/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0074 - val_loss: 1.2326\n",
      "Epoch 135/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0141 - val_loss: 1.2343\n",
      "Epoch 136/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0074 - val_loss: 1.2548\n",
      "Epoch 137/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0097 - val_loss: 1.2178\n",
      "Epoch 138/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0055 - val_loss: 1.2304\n",
      "Epoch 139/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0046 - val_loss: 1.2030\n",
      "Epoch 140/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 1.0026 - val_loss: 1.2308\n",
      "Epoch 141/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9957 - val_loss: 1.2095\n",
      "Epoch 142/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9943 - val_loss: 1.2167\n",
      "Epoch 143/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9966 - val_loss: 1.2211\n",
      "Epoch 144/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9911 - val_loss: 1.2148\n",
      "Epoch 145/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9962 - val_loss: 1.2008\n",
      "Epoch 146/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9864 - val_loss: 1.1906\n",
      "Epoch 147/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9889 - val_loss: 1.2072\n",
      "Epoch 148/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9882 - val_loss: 1.2147\n",
      "Epoch 149/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9858 - val_loss: 1.2242\n",
      "Epoch 150/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9843 - val_loss: 1.1929\n",
      "Epoch 151/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9811 - val_loss: 1.1955\n",
      "Epoch 152/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9814 - val_loss: 1.1787\n",
      "Epoch 153/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9789 - val_loss: 1.2039\n",
      "Epoch 154/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9789 - val_loss: 1.2038\n",
      "Epoch 155/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9787 - val_loss: 1.2006\n",
      "Epoch 156/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9751 - val_loss: 1.2065\n",
      "Epoch 157/220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9766 - val_loss: 1.1934\n",
      "Epoch 158/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9757 - val_loss: 1.1987\n",
      "Epoch 159/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9779 - val_loss: 1.2037\n",
      "Epoch 160/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9709 - val_loss: 1.1958\n",
      "Epoch 161/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9667 - val_loss: 1.1896\n",
      "Epoch 162/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9716 - val_loss: 1.2149\n",
      "Epoch 163/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9663 - val_loss: 1.1850\n",
      "Epoch 164/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9631 - val_loss: 1.1946\n",
      "Epoch 165/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9666 - val_loss: 1.2174\n",
      "Epoch 166/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9664 - val_loss: 1.1887\n",
      "Epoch 167/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9637 - val_loss: 1.2184\n",
      "Epoch 168/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9617 - val_loss: 1.2014\n",
      "Epoch 169/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9625 - val_loss: 1.1740\n",
      "Epoch 170/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9706 - val_loss: 1.1976\n",
      "Epoch 171/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9652 - val_loss: 1.2199\n",
      "Epoch 172/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9619 - val_loss: 1.1994\n",
      "Epoch 173/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9573 - val_loss: 1.1836\n",
      "Epoch 174/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9565 - val_loss: 1.1969\n",
      "Epoch 175/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9585 - val_loss: 1.1976\n",
      "Epoch 176/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9596 - val_loss: 1.1760\n",
      "Epoch 177/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9581 - val_loss: 1.1686\n",
      "Epoch 178/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9588 - val_loss: 1.1815\n",
      "Epoch 179/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9568 - val_loss: 1.1712\n",
      "Epoch 180/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9519 - val_loss: 1.1932\n",
      "Epoch 181/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9558 - val_loss: 1.1734\n",
      "Epoch 182/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9552 - val_loss: 1.1885\n",
      "Epoch 183/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9516 - val_loss: 1.2009\n",
      "Epoch 184/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9532 - val_loss: 1.1784\n",
      "Epoch 185/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9482 - val_loss: 1.1499\n",
      "Epoch 186/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9467 - val_loss: 1.1505\n",
      "Epoch 187/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9461 - val_loss: 1.1471\n",
      "Epoch 188/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9463 - val_loss: 1.1343\n",
      "Epoch 189/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9485 - val_loss: 1.1431\n",
      "Epoch 190/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9442 - val_loss: 1.1406\n",
      "Epoch 191/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9456 - val_loss: 1.1506\n",
      "Epoch 192/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9467 - val_loss: 1.1457\n",
      "Epoch 193/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9415 - val_loss: 1.1459\n",
      "Epoch 194/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9500 - val_loss: 1.1505\n",
      "Epoch 195/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9540 - val_loss: 1.1364\n",
      "Epoch 196/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9444 - val_loss: 1.1404\n",
      "Epoch 197/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9413 - val_loss: 1.1238\n",
      "Epoch 198/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9427 - val_loss: 1.1469\n",
      "Epoch 199/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9413 - val_loss: 1.1356\n",
      "Epoch 200/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9386 - val_loss: 1.1374\n",
      "Epoch 201/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9339 - val_loss: 1.1365\n",
      "Epoch 202/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9385 - val_loss: 1.1430\n",
      "Epoch 203/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9415 - val_loss: 1.1347\n",
      "Epoch 204/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9417 - val_loss: 1.1186\n",
      "Epoch 205/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9436 - val_loss: 1.1442\n",
      "Epoch 206/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9411 - val_loss: 1.1456\n",
      "Epoch 207/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9397 - val_loss: 1.1382\n",
      "Epoch 208/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9387 - val_loss: 1.1422\n",
      "Epoch 209/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9346 - val_loss: 1.1568\n",
      "Epoch 210/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9369 - val_loss: 1.1328\n",
      "Epoch 211/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9397 - val_loss: 1.1444\n",
      "Epoch 212/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9322 - val_loss: 1.1342\n",
      "Epoch 213/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9268 - val_loss: 1.1204\n",
      "Epoch 214/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9330 - val_loss: 1.1292\n",
      "Epoch 215/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9372 - val_loss: 1.1465\n",
      "Epoch 216/220\n",
      "8731/8731 [==============================] - 0s 6us/step - loss: 0.9274 - val_loss: 1.1108\n",
      "Epoch 217/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9338 - val_loss: 1.1191\n",
      "Epoch 218/220\n",
      "8731/8731 [==============================] - 0s 6us/step - loss: 0.9279 - val_loss: 1.1106\n",
      "Epoch 219/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9315 - val_loss: 1.1218\n",
      "Epoch 220/220\n",
      "8731/8731 [==============================] - 0s 5us/step - loss: 0.9276 - val_loss: 1.1082\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('Data/'+region+'_train.csv')\n",
    "df_train['HourOfYear'] = pd.DatetimeIndex(df_train['Date']).dayofyear * 24 + df_train['Hour'] - 24\n",
    "df_dev = pd.read_csv('Data/'+region+'_dev.csv')\n",
    "df_dev2 = pd.read_csv('Data/'+region+'_dev2.csv')\n",
    "df_dev['HourOfYear'] = pd.DatetimeIndex(df_dev['Date']).dayofyear * 24 + df_dev['Hour'] - 24\n",
    "df_dev2['HourOfYear'] = pd.DatetimeIndex(df_dev2['Date']).dayofyear * 24 + df_dev2['Hour'] - 24\n",
    "\n",
    "load_scaler = preprocessing.StandardScaler().fit(df_train['DEMAND'].values.reshape(-1,1))\n",
    "for cols in ['DEMAND', '1D', '2D', '3D', '4D', '5D', '6D', '7D', 'D2', 'D3', 'D4', 'D5']:\n",
    "# for cols in ['DEMAND', 'D1', 'D5', 'DD1', 'DD5', 'W1', 'W5']:\n",
    "    df_train[cols] = load_scaler.transform(df_train.loc[:,cols].values.reshape(-1, 1))\n",
    "    df_dev[cols] = load_scaler.transform(df_dev.loc[:,cols].values.reshape(-1, 1))\n",
    "    df_dev2[cols] = load_scaler.transform(df_dev2.loc[:,cols].values.reshape(-1, 1))\n",
    "\n",
    "for cols in ['Weekday', 'Weekend?', 'Hour', 'Month']:\n",
    "#for cols in ['Weekday', 'Weekend?', 'HourOfYear', 'Day']:\n",
    "    cataScaler = preprocessing.StandardScaler()\n",
    "    df_train[cols] = cataScaler.fit_transform(df_train.loc[:,cols].values.reshape(-1, 1))\n",
    "    df_dev[cols] = cataScaler.transform(df_dev.loc[:,cols].values.reshape(-1, 1))\n",
    "    df_dev2[cols] = cataScaler.transform(df_dev2.loc[:,cols].values.reshape(-1, 1))\n",
    "    \n",
    "df_nn_train = df_train[['DEMAND', '1D', '2D', '3D', '4D', '5D', '6D', '7D','D2', 'D3', 'D4', 'D5', 'Weekday', 'Weekend?', 'Hour', 'Month']]\n",
    "df_nn_dev = df_dev[['DEMAND', '1D', '2D', '3D', '4D', '5D', '6D', '7D','D2', 'D3', 'D4', 'D5', 'Weekday', 'Weekend?', 'Hour', 'Month']]\n",
    "df_nn_dev2 = df_dev2[['DEMAND', '1D', '2D', '3D', '4D', '5D', '6D', '7D','D2', 'D3', 'D4', 'D5', 'Weekday', 'Weekend?', 'Hour', 'Month']]\n",
    "\n",
    "Y = df_nn_train['DEMAND'].values.reshape([-1,1])\n",
    "X = df_nn_train.drop(['DEMAND'], axis = 1).values\n",
    "\n",
    "Y_s = df_nn_dev['DEMAND'].values.reshape([-1,1])\n",
    "X_s = df_nn_dev.drop(['DEMAND'], axis = 1).values\n",
    "\n",
    "Y_r = df_nn_dev2['DEMAND'].values.reshape([-1,1])\n",
    "X_r = df_nn_dev2.drop(['DEMAND'], axis = 1).values\n",
    "\n",
    "model = model(input_shape = (15, ))\n",
    "opt = Adam(lr = 0.0003)\n",
    "model.compile(loss=pinball, optimizer=opt, metrics=[])\n",
    "\n",
    "model.fit(X, Y, batch_size = 1024, epochs=220, validation_data=(X_s, Y_s))\n",
    "\n",
    "Y_r_pred = model.predict(X_r)\n",
    "Y_r_pred = load_scaler.inverse_transform(Y_r_pred)\n",
    "\n",
    "result = np.zeros((Y_r_pred.shape[0],2))\n",
    "for i in range(Y_r_pred.shape[0]):\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(xx, Y_r_pred[i])\n",
    "    result[i,0] = intercept\n",
    "    result[i,1] = slope\n",
    "\n",
    "df_result = pd.DataFrame(result, columns = ['mean', 'std'])\n",
    "df_result.to_csv('results/NN_'+region+'_result'+str(j)+'.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
